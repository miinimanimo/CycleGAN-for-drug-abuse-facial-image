{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython.display import clear_output\n",
    "\n",
    "AUTOTUNE = tf.data.AUTOTUNE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wr_dir = os.getcwd()\n",
    "wr_dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir(wr_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import glob\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# 로컬 이미지 파일 경로 수집\n",
    "train_after_paths = glob.glob(\"C:/Users/user/Desktop/drug_proj/cycle/after/train/*.png\")\n",
    "train_before_paths = glob.glob('C:/Users/user/Desktop/drug_proj/cycle/before/train/*.png')\n",
    "test_after_paths = glob.glob('C:/Users/user/Desktop/drug_proj/cycle/after/test/*.png')\n",
    "test_before_paths = glob.glob('C:/Users/user/Desktop/drug_proj/cycle/before/test/*.png')\n",
    "\n",
    "# tf.data.Dataset으로 데이터셋 생성\n",
    "train_after_ds = tf.data.Dataset.from_tensor_slices(train_after_paths)\n",
    "train_before_ds = tf.data.Dataset.from_tensor_slices(train_before_paths)\n",
    "test_after_ds = tf.data.Dataset.from_tensor_slices(test_after_paths)\n",
    "test_before_ds = tf.data.Dataset.from_tensor_slices(test_before_paths)\n",
    "\n",
    "# 이미지를 읽고 전처리하는 함수 추가\n",
    "def load_image(image_path):\n",
    "    image = tf.io.read_file(image_path)\n",
    "    image = tf.image.decode_png(image, channels=3)  # PNG 이미지용 디코드 함수 사용\n",
    "    return image\n",
    "\n",
    "# 이미지 정규화 함수\n",
    "def normalize(image):\n",
    "    image = tf.cast(image, tf.float32)\n",
    "    image = (image / 127.5) - 1\n",
    "    return image\n",
    "\n",
    "def random_jitter(image):\n",
    "    # 이미지에 배치 차원을 추가하여 4차원 텐서로 변환\n",
    "    image = tf.expand_dims(image, 0)\n",
    "    \n",
    "    # 이미지를 286 x 286 x 3으로 리사이징\n",
    "    image = tf.image.resize(image, [286, 286], method=tf.image.ResizeMethod.NEAREST_NEIGHBOR)\n",
    "    \n",
    "    # 이미지를 랜덤하게 256 x 256 x 3으로 크롭\n",
    "    image = tf.image.random_crop(image, size=[1, 256, 256, 3])\n",
    "    \n",
    "    # 배치 차원을 제거하고 원본 차원으로 복원\n",
    "    image = tf.squeeze(image, [0])\n",
    "    \n",
    "    # 이미지를 랜덤하게 좌우 반전\n",
    "    image = tf.image.random_flip_left_right(image)\n",
    "    \n",
    "    # 이미지에 다시 배치 차원을 추가\n",
    "    image = tf.expand_dims(image, 0)\n",
    "\n",
    "    return image\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# 전처리 함수 적용\n",
    "AUTOTUNE = tf.data.experimental.AUTOTUNE\n",
    "BUFFER_SIZE = 1000\n",
    "BATCH_SIZE = 1\n",
    "\n",
    "def preprocess_image_train(image_path):\n",
    "    image = load_image(image_path)\n",
    "    image = random_jitter(image)\n",
    "    image = normalize(image)\n",
    "    return image\n",
    "\n",
    "def preprocess_image_test(image_path):\n",
    "    image = load_image(image_path)\n",
    "    image = random_jitter(image)\n",
    "    image = normalize(image)\n",
    "    return image\n",
    "\n",
    "train_after = train_after_ds.map(preprocess_image_train, num_parallel_calls=AUTOTUNE).cache().shuffle(BUFFER_SIZE).batch(BATCH_SIZE)\n",
    "train_before = train_before_ds.map(preprocess_image_train, num_parallel_calls=AUTOTUNE).cache().shuffle(BUFFER_SIZE).batch(BATCH_SIZE)\n",
    "test_after = test_after_ds.map(preprocess_image_test, num_parallel_calls=AUTOTUNE).cache().shuffle(BUFFER_SIZE).batch(BATCH_SIZE)\n",
    "test_before = test_before_ds.map(preprocess_image_test, num_parallel_calls=AUTOTUNE).cache().shuffle(BUFFER_SIZE).batch(BATCH_SIZE)\n",
    "\n",
    "# 샘플 이미지 시각화\n",
    "plt.figure(figsize=(8, 8))\n",
    "\n",
    "sample_after = next(iter(train_after))\n",
    "sample_before = next(iter(train_before))\n",
    "\n",
    "plt.subplot(221)\n",
    "plt.title('After')\n",
    "plt.imshow(sample_after[0][0] * 0.5 + 0.5)\n",
    "\n",
    "plt.subplot(222)\n",
    "plt.title('After with random jitter')\n",
    "plt.imshow(tf.squeeze(random_jitter(sample_after[0][0])) * 0.5 + 0.5)\n",
    "\n",
    "plt.subplot(223)\n",
    "plt.title('Before')\n",
    "plt.imshow(sample_before[0][0] * 0.5 + 0.5)\n",
    "\n",
    "plt.subplot(224)\n",
    "plt.title('Before with random jitter')\n",
    "plt.imshow(tf.squeeze(random_jitter(sample_before[0][0])) * 0.5 + 0.5)\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow_addons as tfa\n",
    "from tensorflow_examples.models.pix2pix import pix2pix\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "OUTPUT_CHANNELS = 3\n",
    "\n",
    "generator_g = pix2pix.unet_generator(OUTPUT_CHANNELS, norm_type='instancenorm')\n",
    "generator_f = pix2pix.unet_generator(OUTPUT_CHANNELS, norm_type='instancenorm')\n",
    "\n",
    "discriminator_x = pix2pix.discriminator(norm_type='instancenorm', target=False)\n",
    "discriminator_y = pix2pix.discriminator(norm_type='instancenorm', target=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "to_after = generator_g(tf.squeeze(sample_before, [1]))\n",
    "to_before = generator_f(tf.squeeze(sample_after, [1]))\n",
    "\n",
    "plt.figure(figsize=(8, 8))\n",
    "contrast = 8\n",
    "\n",
    "imgs = [sample_before, to_after, sample_after, to_before]\n",
    "title = ['before', 'to_after', 'after', 'to_before']\n",
    "\n",
    "for i in range(len(imgs)):\n",
    "  plt.subplot(2, 2, i+1)\n",
    "  plt.title(title[i])\n",
    "  if i % 2 == 0:\n",
    "    plt.imshow(tf.squeeze(imgs[i][0]) * 0.5 + 0.5)\n",
    "\n",
    "  else:\n",
    "    plt.imshow(tf.squeeze(imgs[i][0]) * 0.5 * contrast + 0.5)\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(8, 8))\n",
    "\n",
    "plt.subplot(121)\n",
    "plt.title('Is a real after?')\n",
    "plt.imshow(discriminator_y(tf.squeeze(sample_after, [1]))[0, ..., -1], cmap='RdBu_r')\n",
    "\n",
    "plt.subplot(122)\n",
    "plt.title('Is a real before?')\n",
    "plt.imshow(discriminator_x(tf.squeeze(sample_before, [1]))[0, ..., -1], cmap='RdBu_r')\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "LAMBDA = 90"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_obj = tf.keras.losses.BinaryCrossentropy(from_logits=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def discriminator_loss(real, generated):\n",
    "  real_loss = loss_obj(tf.ones_like(real), real)\n",
    "\n",
    "  generated_loss = loss_obj(tf.zeros_like(generated), generated)\n",
    "\n",
    "  total_disc_loss = real_loss + generated_loss\n",
    "\n",
    "  return total_disc_loss * 0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generator_loss(generated):\n",
    "  return loss_obj(tf.ones_like(generated), generated)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_cycle_loss(real_image, cycled_image):\n",
    "  loss1 = tf.reduce_mean(tf.abs(real_image - cycled_image))\n",
    "  \n",
    "  return LAMBDA * loss1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def identity_loss(real_image, same_image):\n",
    "  loss = tf.reduce_mean(tf.abs(real_image - same_image))\n",
    "  return LAMBDA * 0.5 * loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 학습률 스케줄\n",
    "num_steps_per_epoch = 100\n",
    "lr_schedule = tf.keras.optimizers.schedules.PiecewiseConstantDecay(\n",
    "    [float(100 * num_steps_per_epoch)], [0.0002, 0.])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optimizer\n",
    "generator_g_optimizer = tf.keras.optimizers.Adam(learning_rate=lr_schedule)\n",
    "generator_f_optimizer = tf.keras.optimizers.Adam(learning_rate=lr_schedule)\n",
    "discriminator_x_optimizer = tf.keras.optimizers.Adam(learning_rate=lr_schedule)\n",
    "discriminator_y_optimizer = tf.keras.optimizers.Adam(learning_rate=lr_schedule)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint_path = \"./checkpoints/train_90_600\"\n",
    "\n",
    "ckpt = tf.train.Checkpoint(generator_g=generator_g,\n",
    "                           generator_f=generator_f,\n",
    "                           discriminator_x=discriminator_x,\n",
    "                           discriminator_y=discriminator_y,\n",
    "                           generator_g_optimizer=generator_g_optimizer,\n",
    "                           generator_f_optimizer=generator_f_optimizer,\n",
    "                           discriminator_x_optimizer=discriminator_x_optimizer,\n",
    "                           discriminator_y_optimizer=discriminator_y_optimizer)\n",
    "\n",
    "ckpt_manager = tf.train.CheckpointManager(ckpt, checkpoint_path, max_to_keep=5)\n",
    "\n",
    "# if a checkpoint exists, restore the latest checkpoint.\n",
    "if ckpt_manager.latest_checkpoint:\n",
    "  ckpt.restore(ckpt_manager.latest_checkpoint)\n",
    "  print ('Latest checkpoint restored!!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "EPOCHS = 600"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_images(model, test_input):\n",
    "  test_input = tf.squeeze(test_input, [1])  # Remove the image number dimension\n",
    "  prediction = model(test_input)\n",
    "    \n",
    "  plt.figure(figsize=(12, 12))\n",
    "\n",
    "  display_list = [test_input[0], prediction[0]]\n",
    "  title = ['Input Image', 'Predicted Image']\n",
    "\n",
    "  for i in range(2):\n",
    "    plt.subplot(1, 2, i+1)\n",
    "    plt.title(title[i])\n",
    "    # getting the pixel values between [0, 1] to plot it.\n",
    "    plt.imshow(display_list[i] * 0.5 + 0.5)\n",
    "    plt.axis('off')\n",
    "  plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@tf.function\n",
    "def train_step(real_x, real_y):\n",
    "    real_x = tf.squeeze(real_x, axis=[1])  # Squeeze to remove the extra dimension\n",
    "    real_y = tf.squeeze(real_y, axis=[1])  # Squeeze to remove the extra dimension\n",
    "\n",
    "    with tf.GradientTape(persistent=True) as tape:\n",
    "        # Generator G translates X -> Y\n",
    "        # Generator F translates Y -> X.\n",
    "\n",
    "        fake_y = generator_g(real_x, training=True)\n",
    "        cycled_x = generator_f(fake_y, training=True)\n",
    "\n",
    "        fake_x = generator_f(real_y, training=True)\n",
    "        cycled_y = generator_g(fake_x, training=True)\n",
    "\n",
    "        # same_x and same_y are used for identity loss.\n",
    "        same_x = generator_f(real_x, training=True)\n",
    "        same_y = generator_g(real_y, training=True)\n",
    "\n",
    "        disc_real_x = discriminator_x(real_x, training=True)\n",
    "        disc_real_y = discriminator_y(real_y, training=True)\n",
    "\n",
    "        disc_fake_x = discriminator_x(fake_x, training=True)\n",
    "        disc_fake_y = discriminator_y(fake_y, training=True)\n",
    "\n",
    "        # calculate the loss\n",
    "        gen_g_loss = generator_loss(disc_fake_y)\n",
    "        gen_f_loss = generator_loss(disc_fake_x)\n",
    "\n",
    "        total_cycle_loss = calc_cycle_loss(real_x, cycled_x) + calc_cycle_loss(real_y, cycled_y)\n",
    "\n",
    "        # Total generator loss = adversarial loss + cycle loss\n",
    "        total_gen_g_loss = gen_g_loss + total_cycle_loss + identity_loss(real_y, same_y)\n",
    "        total_gen_f_loss = gen_f_loss + total_cycle_loss + identity_loss(real_x, same_x)\n",
    "\n",
    "        disc_x_loss = discriminator_loss(disc_real_x, disc_fake_x)\n",
    "        disc_y_loss = discriminator_loss(disc_real_y, disc_fake_y)\n",
    "\n",
    "    # Calculate the gradients for generator and discriminator\n",
    "    generator_g_gradients = tape.gradient(total_gen_g_loss, generator_g.trainable_variables)\n",
    "    generator_f_gradients = tape.gradient(total_gen_f_loss, generator_f.trainable_variables)\n",
    "\n",
    "    discriminator_x_gradients = tape.gradient(disc_x_loss, discriminator_x.trainable_variables)\n",
    "    discriminator_y_gradients = tape.gradient(disc_y_loss, discriminator_y.trainable_variables)\n",
    "\n",
    "    # Apply the gradients to the optimizer\n",
    "    generator_g_optimizer.apply_gradients(zip(generator_g_gradients, generator_g.trainable_variables))\n",
    "    generator_f_optimizer.apply_gradients(zip(generator_f_gradients, generator_f.trainable_variables))\n",
    "    discriminator_x_optimizer.apply_gradients(zip(discriminator_x_gradients, discriminator_x.trainable_variables))\n",
    "    discriminator_y_optimizer.apply_gradients(zip(discriminator_y_gradients, discriminator_y.trainable_variables))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for epoch in range(EPOCHS):\n",
    "  start = time.time()\n",
    "\n",
    "  n = 0\n",
    "  for image_x, image_y in tf.data.Dataset.zip((train_before, train_after)):\n",
    "    train_step(image_x, image_y)\n",
    "    if n % 10 == 0:\n",
    "      print ('.', end='')\n",
    "    n += 1\n",
    "\n",
    "  clear_output(wait=True)\n",
    "  # is clearly visible.\n",
    "  generate_images(generator_g, sample_before)\n",
    "\n",
    "  if (epoch + 1) % 5 == 0:\n",
    "    ckpt_save_path = ckpt_manager.save()\n",
    "    print ('Saving checkpoint for epoch {} at {}'.format(epoch+1,\n",
    "                                                         ckpt_save_path))\n",
    "\n",
    "  print ('Time taken for epoch {} is {} sec\\n'.format(epoch + 1,\n",
    "                                                      time.time()-start))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run the trained model on the test dataset\n",
    "for inp in test_before.take(5):\n",
    "  generate_images(generator_g, inp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "new_image_path = \"C:/Users/user/Desktop/drug_proj/preprocessed_img1.png\"\n",
    "\n",
    "def load_image(image_path):\n",
    "    image = tf.io.read_file(image_path)\n",
    "    image = tf.image.decode_png(image, channels=3)  # PNG 이미지용 디코드 함수 사용\n",
    "    image = tf.image.resize(image, [256, 256])  # 이미지 크기를 256x256으로 조정\n",
    "    return image\n",
    "\n",
    "# 이미지 로드 및 전처리\n",
    "new_image = load_image(new_image_path)\n",
    "#new_image = random_jitter(new_image)\n",
    "new_image = normalize(new_image)\n",
    "\n",
    "# 이미지 차원 확장 (모델이 배치 입력을 기대하기 때문)\n",
    "new_image = tf.expand_dims(new_image, 0)\n",
    "\n",
    "# 이미지를 모델에 적용\n",
    "output = generator_g(new_image)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 첫 번째 이미지 선택 및 범위 변환\n",
    "output_image = output[0] * 0.5 + 0.5\n",
    "\n",
    "# 이미지 시각화\n",
    "plt.imshow(output_image)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "OUTPUT_CHANNELS = 3\n",
    "generator_g = pix2pix.unet_generator(OUTPUT_CHANNELS, norm_type='instancenorm')\n",
    "\n",
    "checkpoint_path = 'C:/Users/user/Desktop/drug_proj/checkpoints/train_90_600/ckpt-120'\n",
    "checkpoint = tf.train.Checkpoint(generator_g=generator_g)\n",
    "checkpoint.restore(checkpoint_path)\n",
    "\n",
    "generator_g.save('C:/Users/user/Desktop/drug_proj/model/generator_g_train_90_600.h5')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "generator_g"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 모델 사용\n",
    "output = generator_g(new_image, training=Falses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 첫 번째 이미지 선택 및 범위 변환\n",
    "output_image = output[0] * 0.5 + 0.5\n",
    "\n",
    "# 이미지 시각화\n",
    "plt.imshow(output_image)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "drugproj",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
